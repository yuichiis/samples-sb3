# Actor-Critic系アルゴリズムの進化史

## 概要

Actor-Criticは現代の強化学習における最も重要なパラダイムの一つである。本レポートでは、その基本的なアイデアから最新のアルゴリズムまで、主要な手法を時系列順に整理し、それぞれの特徴と進化の文脈を解説する。

---

### 1. **Vanilla Actor-Critic (AC)**

-   **時期**: 1990年代後半〜2000年代初頭
-   **概要**: Actor（方策）が行動を選択し、Critic（価値関数）がその行動の良し悪しをTD誤差として評価し、Actorを更新するという最も基本的な形。
-   **課題**: TD誤差の分散が非常に大きく、学習が不安定になりがちだった。深層学習と組み合わせると、この不安定さはさらに顕著になった。

---

### 2. **DDPG (Deep Deterministic Policy Gradient)**

-   **時期**: 2015年
-   **概要**: DeepMind社が提案。DQNの成功を連続値アクション空間に応用した、深層学習ベースのActor-Critic。
-   **特徴**:
    -   **オフポリシー (Off-Policy)**: 過去の経験を「リプレイバッファ」に蓄積し、学習に何度も再利用する。これにより**サンプル効率が劇的に向上**した。
    -   **決定的方策 (Deterministic Policy)**: Actorは確率分布ではなく、状態に対して唯一の最適な行動を出力する。探索は出力された行動にノイズを加えることで行う。
-   **位置づけ**: 連続値制御タスクにおける深層強化学習のブレークスルーとなった。

---

### 3. **A3C / A2C (Asynchronous / Advantage Actor-Critic)**

-   **時期**: 2016年
-   **概要**: こちらもDeepMind社からの提案。複数のエージェントを並列実行し、経験を非同期（A3C）または同期的（A2C）に集約することで学習を安定化させた。
-   **特徴**:
    -   **オンポリシー (On-Policy)**: DDPGとは対照的に、常に最新のポリシーで収集したデータのみを使用する。
    -   **Advantage関数の利用**: Criticの評価指標として、単純なQ値ではなく「Advantage（その行動が平均よりどれだけ良いか）」を明示的に使用した。
-   **位置づけ**: DDPGとは異なるアプローチで学習を安定させ、オンポリシー系列のアルゴリズムの基礎を築いた。

---

### 4. **PPO (Proximal Policy Optimization)**

-   **時期**: 2017年
-   **概要**: OpenAI社が提案したA2Cの直接的な改良版。A2Cの学習が不安定になる問題を解決することが主目的。
-   **特徴**: ポリシーの更新が一度に大きく変わりすぎないように、更新幅を「クリッピング」という単純かつ強力な手法で制限する。これにより、学習が崩壊するリスクを大幅に低減した。
-   **位置づけ**: **非常に堅牢で実装が容易**なため、現在でも多くの問題で最初に試されるべきベースラインとして絶大な人気を誇る。

---

### 5. **TD3 (Twin Delayed DDPG)**

-   **時期**: 2018年
-   **概要**: DDPGの直接的な改良版。DDPGが学習中にQ値を過大評価してしまうという致命的な欠陥を特定し、それを解決した。
-   **特徴**:
    -   **Twin Critic**: 2つのCriticネットワークを持ち、小さい方の価値推定値を採用することで過大評価を抑制する。
    -   **Delayed Update**: Actorの更新頻度をCriticよりも低くすることで、学習を安定させる。
-   **位置づけ**: DDPGの直接的な上位互換であり、現在DDPGが使われる場面では、ほぼ常にTD3が選択される。

---

### 6. **SAC (Soft Actor-Critic)**

-   **時期**: 2018年
-   **概要**: 「エントロピー最大化」という新しい概念を導入した、オフポリシーのActor-Critic。
-   **特徴**: 報酬を最大化するだけでなく、「行動のランダムさ（エントロピー）」も同時に最大化しようとする。これにより、エージェントは**非常に優れた探索能力**を獲得し、局所最適解に陥りにくくなる。
-   **位置づけ**: TD3と並び、現在の連続値制御タスクにおける**最高峰のアルゴリズムの一つ**。サンプル効率、最終性能、安定性の全てが高いレベルでバランスが取れている。
