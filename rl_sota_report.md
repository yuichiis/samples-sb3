# 現代の強化学習における最先端アルゴリズム

## 概要

「強化学習で一番最先端のアルゴリズムは何か？」という問いに対する答えは、解決したい問題のドメインによって異なる。本レポートでは、現在のトレンドと研究の最前線を踏まえ、主要な3つのカテゴリにおける最先端アルゴリズムをまとめる。

---

### 1. 大規模応用・言語モデルの分野：RLHFとその後継者

ChatGPTのような大規模言語モデル（LLM）の性能を飛躍的に向上させた分野。人間の意図との整合性を高めることが主目的となる。

-   #### **RLHF (Reinforcement Learning from Human Feedback)**
    -   **概要**: 人間のフィードバック（例：「回答AはBより良い」）を用いて「報酬モデル」を学習し、その報酬を最大化するように**PPO**アルゴリズムで言語モデルを微調整する。
    -   **位置づけ**: 現代の対話型AIの基盤を築いた中核技術。堅牢で安定しているPPOが再評価されるきっかけとなった。
    -   **主要論文**: "Training language models to follow instructions with human feedback" (OpenAI, 2022)

-   #### **DPO (Direct Preference Optimization)**
    -   **概要**: RLHFの「報酬モデル学習→強化学習」という2段階のプロセスを統一し、人間の選好データから直接言語モデルのポリシーを最適化する、よりシンプルかつ効率的な手法。
    -   **位置づけ**: 2023年に登場したRLHFの有力な後継者。学習の安定性とシンプルさから、急速に業界標準となりつつある。
    -   **主要論文**: "Direct Preference Optimization: Your Language Model is Secretly a Reward Model" (Stanford, 2023)

---

### 2. 高いサンプル効率と「計画能力」の分野：モデルベース強化学習

少ない試行回数で効率的に学習することが求められるロボティクスなどの分野で主流。環境のシミュレータ（世界モデル）をエージェント自身が学習し、そのモデル内で計画・学習を行う。

-   #### **DreamerV3**
    -   **概要**: 観測データから世界の振る舞いを予測する「世界モデル」を学習する。エージェントは物理世界ではなく、頭の中の「夢」（世界モデルのシミュレーション）の中で行動を繰り返し試行し、最適な行動方針を学習する。
    -   **位置づけ**: 非常に高いサンプル効率を達成し、Minecraftのような複雑なタスクをゼロから学習できる。実世界への応用が最も期待されるアルゴリズムの一つ。
    -   **主要論文**: "Mastering Diverse Domains through World Models" (Google DeepMind, 2023)

-   #### **MuZero**
    -   **概要**: AlphaGo/AlphaZeroの後継。囲碁や将棋のようにルールが既知のタスクだけでなく、Atariのような未知の環境でも、**自らルール（モデル）を学習しながら、モンテカルロ木探索によって最適な行動を計画（探索）**する。
    -   **位置づけ**: モデルベース学習と計画（探索）をシームレスに統合し、様々なドメインで超人的なパフォーマンスを達成した金字塔。
    -   **主要論文**: "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" (DeepMind, 2020)

---

### 3. 既存データの活用分野：オフライン強化学習

安全上の制約などから、実環境で自由に試行錯誤できない状況下で、事前に収集されたログデータのみから学習を行う。

-   #### **CQL (Conservative Q-Learning)**
    -   **概要**: オフライン学習の最大の課題である「分布外（データセットにない）アクションの過大評価」を防ぐため、学習中に保守的な（Conservative）ペナルティを課すことで、データセット内の行動に近い行動を選択するように学習する。
    -   **位置づけ**: オフライン強化学習を実用的なレベルに引き上げた代表的なアルゴリズム。
    -   **主要論文**: "Conservative Q-Learning for Offline Reinforcement Learning" (UC Berkeley, 2020)

-   #### **Decision Transformer**
    -   **概要**: 強化学習の問題を、Transformerを用いた「系列予測問題」として再定義した革新的なアプローチ。「望ましい未来の報酬」「現在の状態」「過去の行動」の系列を入力とし、次に取るべき行動を予測する。
    -   **位置づけ**: 価値関数や方策勾配といった従来の強化学習の枠組みを使わずに高い性能を達成し、大規模モデルの知見を強化学習に応用する新しい方向性を示した。
    -   **主要論文**: "Decision Transformer: Reinforcement Learning via Sequence Modeling" (UC Berkeley, FAIR, Google, 2021)

---

### まとめ表

| カテゴリ | 最先端アルゴリズム | キーワード | 主な応用先 |
| :--- | :--- | :--- | :--- |
| **大規模応用** | **DPO**, RLHF (PPO) | 人間の好み、直接最適化 | 大規模言語モデル (ChatGPTなど) |
| **サンプル効率** | **DreamerV3**, MuZero | 世界モデル、計画、想像 | ロボティクス、ゲームAI |
| **既存データ活用** | **Decision Transformer**, CQL | オフライン、系列予測 | 医療、金融、自動運転 |
